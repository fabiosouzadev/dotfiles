# LLM Local (Ollama)

{{ range .ollama.environments -}}
export {{.}}
{{ end -}}

unset OLLAMA_MLOCK

#Alias
alias -- ollama-start="sudo systemctl start ollama.service"
alias -- ollama-stop="sudo systemctl stop ollama.service"
alias -- ollama-restart="sudo systemctl restart ollama.service"
alias -- ollama-status="sudo systemctl status ollama.service"

alias -- ollama-ui="docker run -d --network=host -v open-webui:/app/backend/data -e OLLAMA_BASE_URL=http://127.0.0.1:11434 --name open-webui --restart always ghcr.io/open-webui/open-webui:main"

# vim: filetype=bash
